
\chapter{Wstęp.disabled}

Produktem pracy będzie biblioteka w języku JavaScript do wykorzystania w zaawansowanych aplikacjach webowych w przeglądarce internetowej.


Kilka uwag do tematu pracy magisterskiej:
\begin{itemize}
	\item Pomysł na temat pracy jest związany z moim doświadczeniem zawodowym zdobytym w firmie DAC System, a problemy technologiczne wskazane poniżej istotnie wpływają na efektywność pracy monitoringu jakości powietrza w Wojewódzkich Inspektoratów Ochrony Środowiska, czyli użytkowników produktu, który rozwijałem we tej firmie.
	\item Część poruszonych problemów została przeze mnie rozwiązana w ramach pracy w firmie DAC System, i zgodnie z sugestią firmy włączę je do swojej pracy.
	\item Mowa tutaj o danych pomiarowych, czyli danych zbieranych przez urządzenia pomiarowe oznaczone stemplem czasowym, zbieranym przez wiele lat co kilka sekund. Jest to zagadnienie związane z tzw. Big Data
	\item Produktem końcowym jest arkusz danych pomiarowych edytowany analogicznie do edycji w programie Excel (zdjęcie poniżej) i wykres liniowy pomagający wyłapać błędne pomiary. Oba produkty dostosowane będą do płynnej obsługi wielkich zbiorów danych pomiarowych.
	\item Główne wyzwania do podjęcia:
		\begin{itemize}
			\item Zaprojektowanie protokołu komunikacyjnego klient-serwer opartego na technologii AJAX, którego zadaniem jest ukrycie przed użytkownikiem faktu, że pracuje on tylko na pewnym wycinku danych. Z tego protokołu będą korzystały komponenty graficznego interfejsu użytkownika.
			\item Optymalizacja procesu renderowania komponentu arkusza danych, by wydajnie obsługiwać lokalne zbiory danych niezależnie od ich rozmiaru.
			\item Zaprojektowanie komponentu wykresów liniowych, pozwalającego na płynne przeglądanie danych pomiarowych, minimalizującego czas oczekiwania użytkownika na dane.
		\end{itemize}
	\item Przypadki użycia które postawiły te wyzwania technologiczne:
		\begin{itemize}
			\item Użytkownik chce płynnie przeglądać dane z dowolnego zakresu czasowego
			\item Kiedy użytkownik powraca do danych już odwiedzonych (np. przewijając wykres w przód lub w tył), dane powinny być odtworzone z pamięci lokalnej.
			\item Niemożliwe jest pobranie wszystkich danych do klienta na jego żądanie
			\item Klient powinien dokonywać predykcji, które fragmenty danych będą za chwilę potrzebne, aby minimalizować czas oczekiwania na pobranie następnego kawałka danych
			\item Klient powinien dbać o zwalnianie zasobów z pamięci na podstawie informacji, które fragmenty danych były użyte najdawniej / najrzadziej.
		\end{itemize}
	\item Rozwiązanie protokołu komunikacji podobne jest do tego stosowanego w mechanizmie używanym przez Mapy Google, w kontekście ładowania kafelków mapy.

\end{itemize}

Oprócz wytworzenia narzędzia w JavaScript, chcę skupić się na tym, jak przygotować swoją pracę dla innych, by z niej mogli skorzystać inni użytkownicy serwisu http://github.com/. Chodzi o czytelność i wiarygodność projektu. Mam tutaj na myśli:
\begin{itemize}
	\item wyodrębnienie mniejszych, samodzielnych i użytecznych  modułów, które rozwiązują pojedyncze problemy programistyczne
	\item wersjonowanie zgodnie z konwencją Semantic Versioning 2.0.0 (http://semver.org/)
	\item jasno zdefiniowane i opisane interfejsy programistyczne
	\item korzystanie ze standardów - dostępność modułów w czasie wykonania: AMD (np. http://requirejs.org/) oraz w środowisku wytwórczym: http://bower.io/ lub http://npmjs.com/
	\item dbałość o kod (m.in brak polskich wstawek w komentarzach), staranne oznaczanie zmian w repozytorium GIT
	\item jakość: pokrycie testami jednostkowymi i funkcjonalnymi, ciągła integracja (ang. contignous integration)
	\item dostosowanie się do obecnych trendów programowania w JavaScript: EcmaScript6, BabelJS, Gulp 


\end{itemize}



>>>>

Niniejsza praca będzie poświęcona głównie ideologii Open Source, czyli tej, w której twórcy oprogramowania dzielą się swoim dziełem udostępniając wszystkie zasoby źródłowe, przyczyniając się tym samym do rozpowszechniania wiedzy i praktyk.

Osoby zainteresowane tym dobrem wspólnym skupiają się wokół specjalistycznych portali internetowych tworząc tym samym specyficzną społeczność. Jedną z nich jest społeczność portalu \textbf{github.com}, który jest obecnie powszechnie znanym zbiorem repozytoriów, miejscem, w którym utrzymywane są najważniejsze i najpopularniejsze biblioteki programistyczne.

Korzyści płynące z korzystania z darmowych, utrzymywanych i rozwijanych, wiarygodnych projektów są zgoła oczywiste. Inaczej jest z korzyściami płynącymi z twórczego wkładu w Otwarte Oprogramowanie. Jedni chcą móc się pochwalić autorstwem powszechnego narzędzia, inni chcę mieć wartościowy wpis w życiorysie, robią to w ramach pracy, a jeszcze inni czują po prostu potrzebę podzielenia się swoim dziełem z innymi.
<<< aspekt zopensourcownani
W tej pracy postawię się w roli programisty chcącego utworzyć nowy projekt open source, który będzie współrozwijany przez społeczność GitHuba, zyska na popularności i nabierze własnego rytmu. Niestety, można się domyśleć, że do osiągnięcia opensourcowego sukcesu nie wystarczy załadować swój "idealny" projekt do repozytorium GitHuba. Nawet najlepsze pomysły, którmi chce się podzielić twórczy programista, są skazane na brak zainteresowania, jeśli nie są dostosowane do wymagań innych użytkowników. Aby sprawdzić, czym powinien charakteryzować się nowy, nieznany jeszcze projekt, będę musiał postawić się również w roli programisty, który przypadkiem znalazł ciekawą bibliotekę nadającą się do jego projektu, ale która w ogóle nie jest znana, co wiąże się zwiększonym ryzykiem porażki całego projektu. Tylko wtedy, gdy sam użyje tej biblioteki, może stwierdzić, że warto poświęcić swój czas na jej rozwijanie.

Postaram się znaleźć dobre odpowiedzi na poniższe pytania:
\begin{itemize}
\item kiedy bibliotekę otwartoźródłową społeczność uważa za godną użycia,
\item jakie są zagrożenia płynące z użycia niewiarygodnej biblioteki,
\item czego wymaga się od bibliotek rozwijanych przez innych.
\end{itemize}
Zanim rozpocznę tworzenie projektu open source na portalu github.com, przeprowadzę krótką analizę, by dowiedzieć się, czym charakteryzowały się w swojej początkowej fazie projekty na tym portalu, które obecnie zyskały na popularności. Ponieważ biblioteka, którą chcę tworzyć, będzie napisana w języku JavaScript, w analizie ograniczę się do małych bibliotek również napisanych w tym języku. Dzięki temu, z zebranych spostrzeżeń będę mógł wysnuć wnioski pomocne przy planowaniu projektu.

Na wstępie warto zaznaczyć, że pomysł realizowany w projekcie powinien być innowacyjny, nie powinien być tylko alternatywą istniejącego rozwiązania, gdyż można się domyśleć, że ludzie mając porównanie z popularną biblioteką, będą kierowali się raczej tym ostatnim w kryteriach swojego wyboru.
>>>>odtąd prawdziwy wstęp
Projekt będzie miał na celu wytworzenie narzędzia programistycznego rozwiązującego problemy występujące w bliskiej mi branży telemetrii i monitoringu środowiska. Problemy te poznałem zdobywając doświadczenie w firmie DAC System, gdzie brałem udział w wytwarzaniu systemu CAS14, tzw. \textbf{grubego klienta} systemu monitoringu jakości powietrza dla ośmiu Wojewódzkich Inspektoratów Ochrony Środowiska, zamawianych przez Główny Inspektorat Ochrony Środowiska w zamówieniu \textsl{ZP/DM/0811-07-EOG/01/2013/MZ}.

Część tych problemów rozwiązałem w tamtym systemie, dlatego za zgodą i jednocześnie sugestią ze strony firmy DAC System, przeniosę część rozwiązań do narzędzia otwartego.

%Blabla jak ważne są dane pomiarowe i efektywne ich użycie, potem że skoro chcę rozwiązać problemy, które nie wydają się być jeszcze dobrze rozwiązane, warto się podzielić ze światem, ale to nie jest takie oczywiste.

\section{Słowa kluczowe}
scalable data
big data
time series data
loading, prefetching, caching
rendering, optimizing
render time, load time, latency
user experience UX
on-demand data loading, preloading
dependency cache
render cache
2d drawing cache performance
javascript performance tricks
webkit performance
wielkoskalowe dane pomiarowe
\section{Cel pracy}

1. Warstwa danych będąca pomiędzy warstwą interfejsu użytkownika a warstwą dostępu do danych AJAX, zarządzająca zasobami zewnętrznymi w celu zminimalizowania czasu reakcji na akcje użytkownika na podstawie predykcji oraz mechanizmów cache


Description of the solution.
It is an abstract layer between client data connector and GUI component on client side javascript application.

Features:
* manages 1D (time-based series) or 2D (maps, etc)

\subsection{Cel biznesowy i Open Source}

\subsubsection*{Cel biznesowy}
%wydajny monitoring i walidacja danych pomiarowych
\subsubsection*{Cel Open Source}
%rozpowszechnienie, interakcja z githubem, kontrybucje

\subsection{Produkt pracy}
%biblioteka programistyczna i przykładowa aplikacja demonstracyjna (użycie biblioteki)
%na jakich licencjach wydano produkty - i dlaczego

\subsection{Charakterystyka Open Source}

%definicja sukcesu projektu otwartoźródłowego

\section{Motywacja}

%Przyczyny podjęcia się tematu
Jest dużo bibliotek, które szczycą się wydajną obsługą wielkich zbiorów danych, niestety żadna nie bierze pod uwagę doładowywania fragmentów danych on-demand, żadnej predykcji. Takie rozwiązania wymagają rekonfiguracji komponentu co prowadzi do utracenia kontekstu operacji użytkownika.
Wszystkie rozwiązania dotyczą momentu, w którym mamy już dane, 
my zajmiemy się mechanizmami efektywnego dostarzczania tych danych.

Nie ma rozwiązania takiego z edycją danych poprzez integrację modułów




\subsection{Rola responsywności systemu}
example citations
miller: \cite{Miller1968}
jakob nielsen: \cite{jakob1993usability}
html5 perf: \cite{html5-perf}
dont let me think: \cite{dont-let-me-think}
es6 standard: \cite{es6-standard}
tim slatcher: \cite{conference}
illusion of time conf: \cite{illusion-of-time}
illusion of time article \cite{why-performance-matters-part1}
illusion of time article \cite{why-performance-matters-part2}
google speed: \cite{google-speed}
interakcja człowiek komputer: \cite{interakcja}
js best practises: \cite{js-best-practises}
hauser12: \cite{Hauser12VisTutorial}
keim 2006 challenges: \cite{keim2006challenges}
thomas 2009 challenges: \cite{thomas2009challenges}
geospatial: \cite{sample2010tile}
forecache: \cite{forecache}
visual latency: \cite{effect-interactive-latency}
prefetching exploration: \cite{prefetching-visual-data-exploration}

markov: \cite{lee2002adaptation}
\subsection{Monitoring środowiska}
%implementacja wkleszczona w system CAS14, bardzo pod klienta, a warto takim czymś podzielić się ze światem, bo tego nie ma. implementacja korzysta z bibliotek dostępnych w tamtym systemie, a których licencja nie pozwala na otwarcie kodu źródłowego

Wymagania - nieskończoność danych - z założenia nie ładujemy wszystkiego na raz,
agregacje dopasowane do aktualnego zoomu, store uczy sie jakie ruchy robi uzytkownik i tak robi invalidację cache / prefetching



\subsection{Charakterystyka eksploracji w wizualnej analizie danych}
Nawiązać do:
http://istc-bigdata.org/index.php/forecache-raising-the-bar-in-big-data-visual-exploration/


In many discussions with scientists across a variety of specialties, we have found that interactive visualizations are important tools for helping people make sense of massive amounts of data. In particular, interactive visualizations are critical in the early stages of data analysis, when a scientist is browsing a new, unfamiliar dataset. In a research project, we studied how scientists explore dense multidimensional arrays, such as satellite imagery, so we could learn about technical barriers in their way.

We have observed that scientists explore dense array data in a particular way. First, they browse the data using a coarse-grained aggregated view (i.e., a low-resolution view), searching for interesting regions to analyze in more detail. Once they find a region of interest (or ROI), they “zoom in” by retrieving a fine-grained view (i.e., high-resolution view) of this smaller region from the dataset. Using detail-on-demand interfaces in their exploration tools, scientists can thus apply panning and zooming interactions to explore large arrays.

However, most interactive exploration tools are unable to scale up to massive datasets. Therefore, one major goal of this project, and in my thesis work, is to make visual exploration of large arrays interactive, where the user (e.g., a scientist) receives visual feedback from the system within acceptable response time guarantees (e.g., within 500ms or less). However, a critical challenge in this project is that database management systems are not designed for retrieving results at interactive speeds, making them too slow to provide the fast preliminary results needed by a scalable interactive exploration interface.



\subsection{Analiza istniejących rozwiązań}

pokazać metodykę sprawdzenie, jakie query, jakie 20 best frameworks, jakie popularne toole i w tabelce pokazac

* http://leafletjs.com/ - tiles loading

* http://www.createjs.com/preloadjs
bibllioteka w ogóle podejmująca się tematyi preloadingu

* http://www.zingchart.com/features/big-data-charts/

 Render Interactive Big Data Charts
 We think ZingChart is the best big data JavaScript charting library out there. But if you want to compare us to other libraries before making your choice, here’s your chance!

Customizable Sampling
When rendering large datasets, data sampling can improve render speeds. In fact, sometimes it's the only way to accurately draw the data in a confined space. ZingChart provides multiple options for data sampling including exact, default sampling, smart sampling, and custom sampling.

* https://nexts.github.io/Clusterize.js/
alternatywa do grida, znowu, nie ma wsparcia dla loading-on-demand
\subsection{Stan wyjściowy}
%arkusz danych w ExtJS, ściśle zależny od innych modułów aplikacji CAS14 firmy DAC System, co dokładnie robi kod i co kopiuje do magisterski, a czego nie, co dodatkowo będzie trzeba zrobić
%jak wygląda inicjalnie repozytorium GitHuba

\section{Główne wyzwania}
%zainteresowanie społeczności programistów na portalu GitHub
%kolaboracja - rezerwowanie obszarów na wyłączność, obszary niekonfliktowe
%powiadomienia o zmianach
%pokrycie testami - wiarygodność w open source
%lokalna historia - cofnij i powtórz
%ux arkusza
%wielkość danych
% * efektywne przeglądanie danych historycznych
% * cache i prefetch
% * infinity data jak w google maps
\section{Aspekt badawczy}
%było dużo we wstępie
\section{Wykorzystane narzędzia}
% spring boot, es6, angular, webpack, babel
%spring boot, es6, angular, webpack, babel
%github, może heroku, może jakiś remote build system
%Java/Javascript/compass
\section{Wykorzystane wzorce projektowe}
